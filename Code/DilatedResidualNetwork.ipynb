{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRN50_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo1I0gk7b9Ji",
        "colab_type": "code",
        "outputId": "a2fac32e-0f71-4c06-b40a-6be92a62eecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgN9cyHtd1x2",
        "colab_type": "text"
      },
      "source": [
        "###Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFDFEro0ntJ",
        "colab_type": "code",
        "outputId": "c4df37ae-acec-4bfc-f9ac-1a0f9d0e7693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (1.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrDLGPXsuXHA",
        "colab_type": "code",
        "outputId": "836e04cf-30a0-4863-e71c-2a3fd28d435c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2aRROmd42d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "from math import ceil, floor, log\n",
        "import cv2\n",
        "from keras.models import model_from_json\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import sys\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation, ZeroPadding2D, BatchNormalization, MaxPooling2D, Add, AveragePooling2D\n",
        "from sklearn.model_selection import ShuffleSplit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjOjzIHncX7g",
        "colab_type": "text"
      },
      "source": [
        "###Declaring Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmcqp6bYcXAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ALL_LABELS = 'gdrive/My Drive/stage_1_train.csv'\n",
        "TEST_IMAGES_DIR = 'gdrive/My Drive/RSNA Sample 10 GB attempt 3/'\n",
        "TEST_LABELS = 'gdrive/My Drive/test.csv'\n",
        "TRAIN_IMAGES_DIR = 'gdrive/My Drive/RSNA Sample 10 GB attempt 3/'\n",
        "TRAIN_LABELS = 'gdrive/My Drive/train.csv'\n",
        "#For Saving\n",
        "WEIGHTS_SAVE_DIR = 'gdrive/My Drive/Weights/'\n",
        "WEIGHT_SAVING_NAME = 'DRN_Hidden_lr1e-6'\n",
        "#For Loading\n",
        "SHORTLIST_WEIGHTS_DIR = 'gdrive/My Drive/Weights/'\n",
        "WEIGHT_LOAD_NAME = 'DRN_Hidden_lr1e-602.hdf5'\n",
        "\n",
        "#Sampling (Between 0 and 1)\n",
        "KEEP_PROB_0 = 1\n",
        "KEEP_PROB_1 = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTdWUVMGlovr",
        "colab_type": "text"
      },
      "source": [
        "###Train Test Split (Needed to be run only once on a platform)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpdTevxnlrYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(ALL_LABELS)\n",
        "df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)    \n",
        "duplicates_to_remove = [\n",
        "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
        "        312468,  312469,  312470,  312471,  312472,  312473,\n",
        "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
        "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
        "    ]\n",
        "    \n",
        "df = df.drop(index=duplicates_to_remove)\n",
        "df = df.reset_index(drop=True)\n",
        "df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pGlkbht0-6b",
        "colab_type": "code",
        "outputId": "8d8a42d6-73ab-45d5-d21b-4f30e01f4d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "###Caution for Google drive data (Avoid for other platforms)\n",
        "root_path = 'gdrive/My Drive/RSNA Sample 10 GB attempt 3/'\n",
        "import os\n",
        "a = os.listdir(root_path)\n",
        "a = [item.rstrip('.dcm') for item in a]\n",
        "df = df.stack().reset_index()\n",
        "df = df[df['Image'].isin(a)]\n",
        "df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGMAMkbhvo20",
        "colab_type": "code",
        "outputId": "4386a508-eed1-4695-8aea-c905e84548da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test  = train_test_split(df, test_size=0.2, random_state=1)\n",
        "test = test.stack().reset_index()\n",
        "test.insert(loc=0, column='ID', value=test['Image'].astype(str) + \"_\" + test['Diagnosis'])\n",
        "test = test.drop([\"Image\", \"Diagnosis\"], axis=1)\n",
        "train = train.stack().reset_index()\n",
        "train.insert(loc=0, column='ID', value=train['Image'].astype(str) + \"_\" + train['Diagnosis'])\n",
        "train = train.drop([\"Image\", \"Diagnosis\"], axis=1)\n",
        "print(len(train),len(test))\n",
        "test.to_csv(TEST_LABELS)\n",
        "train.to_csv(TRAIN_LABELS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62694 15678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u8JaZ3-d7WM",
        "colab_type": "text"
      },
      "source": [
        "###Windowing (BSB & Manual)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BLB5UzyeMBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_dcm(dcm):\n",
        "    x = dcm.pixel_array + 1000\n",
        "    px_mode = 4096\n",
        "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
        "    dcm.PixelData = x.tobytes()\n",
        "    dcm.RescaleIntercept = -1000\n",
        "\n",
        "def window_image(dcm, window_center, window_width):\n",
        "    \n",
        "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
        "        correct_dcm(dcm)\n",
        "    \n",
        "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "    img_min = window_center - window_width // 2\n",
        "    img_max = window_center + window_width // 2\n",
        "    img = np.clip(img, img_min, img_max)\n",
        "\n",
        "    return img\n",
        "\n",
        "def bsb_window(dcm):\n",
        "    brain_img = window_image(dcm, 40, 80)\n",
        "    subdural_img = window_image(dcm, 80, 200)\n",
        "    soft_img = window_image(dcm, 40, 380)\n",
        "    \n",
        "    brain_img = (brain_img - 0) / 80\n",
        "    subdural_img = (subdural_img - (-20)) / 200\n",
        "    soft_img = (soft_img - (-150)) / 380\n",
        "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
        "\n",
        "    return bsb_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79WITeZ-hIhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def manual_window(dcm, window_center, window_width):\n",
        "#     img = window_image(dcm, window_center, window_width)\n",
        "#     img = (img - (window_center - window_width/2) / \n",
        "#     manual_img = np.array([img, img, img]).transpose(1,2,0)\n",
        "\n",
        "#     return manual_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-MUvX5_eURX",
        "colab_type": "text"
      },
      "source": [
        "###Read images (Change Windowing here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xum-0F-feaxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _read(path, desired_size):\n",
        "    \"\"\"Will be used in DataGenerator\"\"\"\n",
        "    \n",
        "    dcm = pydicom.dcmread(path)\n",
        "    \n",
        "    try:\n",
        "        img = bsb_window(dcm)\n",
        "        #img = manual_window(dcm, 40, 80) # Specify window center, window_width here\n",
        "    except:\n",
        "        img = np.zeros(desired_size)\n",
        "    \n",
        "    \n",
        "    img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n",
        "    \n",
        "    return img\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N-vTqg0ecJE",
        "colab_type": "text"
      },
      "source": [
        "###Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q7jxulxejlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n",
        "                 img_dir=TRAIN_IMAGES_DIR, *args, **kwargs):\n",
        "\n",
        "        self.list_IDs = list_IDs\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.img_dir = img_dir\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
        "        \n",
        "        if self.labels is not None:\n",
        "            X, Y = self.__data_generation(list_IDs_temp)\n",
        "            return X, Y\n",
        "        else:\n",
        "            X = self.__data_generation(list_IDs_temp)\n",
        "            return X\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        \n",
        "        \n",
        "        if self.labels is not None: # for training phase we undersample and shuffle\n",
        "            # keep probability of any=0 and any=1\n",
        "            keep_prob = self.labels.iloc[:, 0].map({0: KEEP_PROB_0, 1: KEEP_PROB_1})\n",
        "            keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
        "            self.indices = np.arange(len(self.list_IDs))#[keep]\n",
        "            np.random.shuffle(self.indices)\n",
        "        else:\n",
        "            self.indices = np.arange(len(self.list_IDs))\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        X = np.empty((self.batch_size, *self.img_size))\n",
        "        \n",
        "        if self.labels is not None: # training phase\n",
        "            Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
        "        \n",
        "            for i, ID in enumerate(list_IDs_temp):\n",
        "                X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
        "                Y[i,] = self.labels.loc[ID].values\n",
        "        \n",
        "            return X, Y\n",
        "        \n",
        "        else: # test phase\n",
        "            for i, ID in enumerate(list_IDs_temp):\n",
        "                X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
        "            \n",
        "            return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPAF0kP8eplR",
        "colab_type": "text"
      },
      "source": [
        "###Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1oyIW2letUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def _normalized_weighted_average(arr, weights=None):\n",
        "    \"\"\"\n",
        "    A simple Keras implementation that mimics that of \n",
        "    numpy.average(), specifically for this competition\n",
        "    \"\"\"\n",
        "    \n",
        "    if weights is not None:\n",
        "        scl = K.sum(weights)\n",
        "        weights = K.expand_dims(weights, axis=1)\n",
        "        return K.sum(K.dot(arr, weights), axis=1) / scl\n",
        "    return K.mean(arr, axis=1)\n",
        "\n",
        "\n",
        "def weighted_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Will be used as the metric in model.compile()\n",
        "    ---------------------------------------------\n",
        "    \n",
        "    Similar to the custom loss function 'weighted_log_loss()' above\n",
        "    but with normalized weights, which should be very similar \n",
        "    to the official competition metric:\n",
        "        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n",
        "    and hence:\n",
        "        sklearn.metrics.log_loss with sample weights\n",
        "    \"\"\"\n",
        "    \n",
        "    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
        "    \n",
        "    eps = K.epsilon()\n",
        "    \n",
        "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
        "\n",
        "    loss = -(        y_true  * K.log(      y_pred)\n",
        "            + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
        "    \n",
        "    loss_samples = _normalized_weighted_average(loss, class_weights)\n",
        "    \n",
        "    return K.mean(loss_samples)\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
        "    K.get_session().run(tf.local_variables_initializer())\n",
        "    return auc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhiR8zRYewBD",
        "colab_type": "text"
      },
      "source": [
        "###Model (Change hidden layers here) (Remove cross validation here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz8TygiNT9E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identity_block(X, f, filters, stage, block, dilation=1):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    X_shortcut = X\n",
        "    \n",
        "    X = Conv2D(filters = F1, kernel_size = (1,1), strides = (1,1), \\\n",
        "                padding = 'valid', dilation_rate=dilation, name = conv_name_base+'2a')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = Conv2D(filters = F2, kernel_size = (f,f), strides = (1,1), \\\n",
        "                padding = 'same', dilation_rate=dilation, name = conv_name_base+'2b')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters = F3, kernel_size = (1,1), strides = (1,1), \\\n",
        "                padding = 'valid', dilation_rate=dilation, name = conv_name_base+'2c')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2, dilation=1):\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    X_shortcut = X\n",
        "    \n",
        "    X = Conv2D(filters = F1, kernel_size = (1,1), strides = (s,s), \\\n",
        "                padding = 'valid', dilation_rate=dilation, name = conv_name_base+'2a')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    X = Conv2D(filters = F2, kernel_size = (f,f), strides = (1,1), \\\n",
        "                padding = 'same', dilation_rate=dilation, name = conv_name_base+'2b')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = Conv2D(filters = F3, kernel_size = (1,1), strides = (1,1), \\\n",
        "                padding = 'valid', dilation_rate=dilation, name = conv_name_base+'2c')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    X_shortcut = Conv2D(filters=F3, kernel_size=(1,1), strides=(s,s), padding='valid',\\\n",
        "                        name=conv_name_base + '1')(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "    \n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLmM06Moe0AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "class MyDeepModel:\n",
        "\n",
        "    \n",
        "  def __init__(self, engine, input_dims, batch_size=5, num_epochs=4, learning_rate=1e-3, \n",
        "                decay_rate=1.0, decay_steps=1, weights=\"imagenet\", verbose=1):\n",
        "        self.engine = engine\n",
        "        self.input_dims = input_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.weights = weights\n",
        "        self.verbose = verbose\n",
        "        self._build()\n",
        "\n",
        "  def _build(self):\n",
        "        engine = self.engine(include_top=False, weights=self.weights, input_shape=self.input_dims, \n",
        "                            backend = keras.backend, layers = keras.layers,\n",
        "                            models = keras.models, utils = keras.utils)\n",
        "        \n",
        "        # for layer in engine.layers[:80]:\n",
        "        #   layer.trainable = False\n",
        "\n",
        "        resnet_stage3 = engine.layers[80]\n",
        "\n",
        "        X = convolutional_block(resnet_stage3.output, f=3, filters=[256, 256, 1024], stage=4, block='a', s=1, dilation=1)\n",
        "        X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b', dilation=2)\n",
        "        X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c', dilation=2)\n",
        "        X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d', dilation=2)\n",
        "        X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e', dilation=2)\n",
        "        X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f', dilation=2)\n",
        "\n",
        "        X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=1, dilation=2)\n",
        "        X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b', dilation=4)\n",
        "        X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c', dilation=4)\n",
        "\n",
        "        X = Flatten()(X)\n",
        "        out = keras.layers.Dense(6, activation=\"sigmoid\", name='fc6')(X)\n",
        "\n",
        "        # Create model\n",
        "\n",
        "        self.model = keras.models.Model(inputs=engine.input, outputs=out, name='Dilated ResNet50')\n",
        "\n",
        "        self.model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=[weighted_loss, auc])\n",
        "  \n",
        "\n",
        "  def fit_model(self, train_df, valid_df):\n",
        "      \n",
        "    # callbacks\n",
        "#For kaggle krnel os.chdir to working dir\n",
        "        checkpointer = keras.callbacks.ModelCheckpoint(filepath=WEIGHTS_SAVE_DIR+WEIGHT_SAVING_NAME+'{epoch:02d}.hdf5' , verbose=1, save_weights_only=True, save_best_only=False, monitor='val_loss')\n",
        "    #Change back to the input directory\n",
        "        scheduler = keras.callbacks.LearningRateScheduler(lambda epoch: self.learning_rate * pow(self.decay_rate, floor(epoch / self.decay_steps)))\n",
        "        \n",
        "        return self.model.fit_generator(\n",
        "            DataGenerator(\n",
        "                train_df.index, \n",
        "                train_df, \n",
        "                self.batch_size, \n",
        "                self.input_dims, \n",
        "                TRAIN_IMAGES_DIR\n",
        "            ),\n",
        "            validation_data = DataGenerator(\n",
        "                valid_df.index, \n",
        "                valid_df, \n",
        "                self.batch_size, \n",
        "                self.input_dims, \n",
        "                TRAIN_IMAGES_DIR\n",
        "            ),\n",
        "            epochs=self.num_epochs,\n",
        "            verbose=self.verbose,\n",
        "            use_multiprocessing=True,\n",
        "            workers=4,\n",
        "            callbacks=[scheduler, checkpointer]\n",
        "          )\n",
        "\n",
        "  def save(self, path):\n",
        "        self.model.save_weights(path)\n",
        "  \n",
        "  def load(self, path):\n",
        "        self.model.load_weights(path)\n",
        "\n",
        "  def predictions(self, test_df, batch_size_pred=8):\n",
        "        self.test_df= test_df\n",
        "        self.batch_size_pred = batch_size_pred\n",
        "        test_preds = self.model.predict_generator(DataGenerator(self.test_df.index, None, self.batch_size_pred, self.input_dims, TEST_IMAGES_DIR), verbose=1)\n",
        "        return test_preds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nAhogoxe06A",
        "colab_type": "text"
      },
      "source": [
        "###Read and convert labels dataset to our format (Remove the duplicates when running on GCP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njSmYgRme-hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_trainset(filename=TRAIN_LABELS):\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
        "    \n",
        "    duplicates_to_remove = [\n",
        "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
        "        312468,  312469,  312470,  312471,  312472,  312473,\n",
        "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
        "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
        "    ]\n",
        "    \n",
        "#    df = df.drop(index=duplicates_to_remove)\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def read_testset(filename=TEST_LABELS):\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
        "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
        "    duplicates_to_remove = [\n",
        "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
        "        312468,  312469,  312470,  312471,  312472,  312473,\n",
        "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
        "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
        "    ]\n",
        "    \n",
        "    #df = df.drop(index=duplicates_to_remove)\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
        "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
        "    \n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik6BjMUufPNh",
        "colab_type": "text"
      },
      "source": [
        "###Hyperparameters (Most Important Part) - Run this only when training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PAyYSQB_OZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To avoid tons of warning that are going to appear in training the model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi58dXcEfT5r",
        "colab_type": "code",
        "outputId": "bee0f627-b1ad-4c86-a24b-33919f4f4f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "#Read train set\n",
        "df = read_trainset()\n",
        "\n",
        "# train set and validation set (Change number of splits)\n",
        "ss = ShuffleSplit(n_splits=3, test_size=0.2, random_state=42).split(df.index)\n",
        "\n",
        "train_idx, valid_idx = next(ss)\n",
        "\n",
        "model = MyDeepModel(engine=ResNet50, input_dims=(224, 224, 3), batch_size=16, learning_rate=1e-6,\n",
        "                    num_epochs=3, decay_rate=0.8, decay_steps=1, weights=\"imagenet\", verbose=1)\n",
        "model.load(SHORTLIST_WEIGHTS_DIR+WEIGHT_LOAD_NAME)\n",
        "history = model.fit_model(df.iloc[train_idx], df.iloc[valid_idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/3\n",
            "523/523 [==============================] - 1036s 2s/step - loss: 0.3759 - weighted_loss: 0.3972 - auc: 0.8481 - val_loss: 0.4709 - val_weighted_loss: 0.4994 - val_auc: 0.8430\n",
            "\n",
            "Epoch 00001: saving model to gdrive/My Drive/McCombs/Fall2019/APM/Weights/DRN_Hidden_lr1e-601.hdf5\n",
            "Epoch 2/3\n",
            "523/523 [==============================] - 945s 2s/step - loss: 0.2986 - weighted_loss: 0.3172 - auc: 0.8556 - val_loss: 0.4709 - val_weighted_loss: 0.5001 - val_auc: 0.8629\n",
            "\n",
            "Epoch 00002: saving model to gdrive/My Drive/McCombs/Fall2019/APM/Weights/DRN_Hidden_lr1e-602.hdf5\n",
            "Epoch 3/3\n",
            "523/523 [==============================] - 943s 2s/step - loss: 0.2528 - weighted_loss: 0.2685 - auc: 0.8714 - val_loss: 0.4730 - val_weighted_loss: 0.4992 - val_auc: 0.8782\n",
            "\n",
            "Epoch 00003: saving model to gdrive/My Drive/McCombs/Fall2019/APM/Weights/DRN_Hidden_lr1e-603.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
